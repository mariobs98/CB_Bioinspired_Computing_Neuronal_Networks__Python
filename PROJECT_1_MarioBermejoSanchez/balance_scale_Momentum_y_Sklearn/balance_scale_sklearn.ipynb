{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#En esta red trato de averiguar si la balanza esta hacia la derecha, la izq o esta balanceada\n",
    "\n",
    "#Lo realizo con sklearn esta vez\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES DE ACTIVACION\n",
    "class function(object):\n",
    "    def __init__(self,funcion,derivative=None):\n",
    "        self.F=funcion\n",
    "        self.D=derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625, 4)\n",
      "(625, 3)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "with open('balance_scale.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile)\n",
    "    for row in readCSV:\n",
    "        #row=row[0].split(\",\")\n",
    "        X.append([float(row[0])/10,float(row[1])/10,float(row[2])/10,float(row[3])/10])#normalizar entrada, tendremos 4 neuronas., 4 campos del dataset\n",
    "\n",
    "        if row[4]=='B':#normalizar salidas, viene ddo siempre por los data set....Pandas y sklearn lo hacen\n",
    "            Y.append([1,0,0]) #balanced\n",
    "        if row[4]=='R':\n",
    "            Y.append([0,1,0]) #right\n",
    "        if row[4]=='L':\n",
    "            Y.append([0,0,1]) #left\n",
    "\n",
    "\n",
    "Xtrain=np.array(X)#vector de entradas\n",
    "Ytrain=np.array(Y)#vector de salida\n",
    "\n",
    "print(np.shape(Xtrain))\n",
    "print(np.shape(Ytrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.39310583\n",
      "Iteration 2, loss = 2.34046961\n",
      "Iteration 3, loss = 2.28991786\n",
      "Iteration 4, loss = 2.24060204\n",
      "Iteration 5, loss = 2.19355086\n",
      "Iteration 6, loss = 2.14803755\n",
      "Iteration 7, loss = 2.10440538\n",
      "Iteration 8, loss = 2.06392382\n",
      "Iteration 9, loss = 2.02419796\n",
      "Iteration 10, loss = 1.98588705\n",
      "Iteration 11, loss = 1.94709186\n",
      "Iteration 12, loss = 1.90948656\n",
      "Iteration 13, loss = 1.87191244\n",
      "Iteration 14, loss = 1.83526344\n",
      "Iteration 15, loss = 1.79974028\n",
      "Iteration 16, loss = 1.76552378\n",
      "Iteration 17, loss = 1.73579759\n",
      "Iteration 18, loss = 1.71174751\n",
      "Iteration 19, loss = 1.69104629\n",
      "Iteration 20, loss = 1.67163320\n",
      "Iteration 21, loss = 1.65607540\n",
      "Iteration 22, loss = 1.64274284\n",
      "Iteration 23, loss = 1.63169845\n",
      "Iteration 24, loss = 1.62276290\n",
      "Iteration 25, loss = 1.61560652\n",
      "Iteration 26, loss = 1.60907913\n",
      "Iteration 27, loss = 1.60314755\n",
      "Iteration 28, loss = 1.59773444\n",
      "Iteration 29, loss = 1.59103652\n",
      "Iteration 30, loss = 1.58303452\n",
      "Iteration 31, loss = 1.57411084\n",
      "Iteration 32, loss = 1.56607459\n",
      "Iteration 33, loss = 1.55829466\n",
      "Iteration 34, loss = 1.54979453\n",
      "Iteration 35, loss = 1.54101986\n",
      "Iteration 36, loss = 1.53215705\n",
      "Iteration 37, loss = 1.52263091\n",
      "Iteration 38, loss = 1.51248980\n",
      "Iteration 39, loss = 1.50196006\n",
      "Iteration 40, loss = 1.49102142\n",
      "Iteration 41, loss = 1.47926164\n",
      "Iteration 42, loss = 1.46708756\n",
      "Iteration 43, loss = 1.45459523\n",
      "Iteration 44, loss = 1.44168853\n",
      "Iteration 45, loss = 1.42786766\n",
      "Iteration 46, loss = 1.41363273\n",
      "Iteration 47, loss = 1.39874651\n",
      "Iteration 48, loss = 1.38359378\n",
      "Iteration 49, loss = 1.36755806\n",
      "Iteration 50, loss = 1.35093776\n",
      "Iteration 51, loss = 1.33357742\n",
      "Iteration 52, loss = 1.31611993\n",
      "Iteration 53, loss = 1.29697916\n",
      "Iteration 54, loss = 1.27786630\n",
      "Iteration 55, loss = 1.25817057\n",
      "Iteration 56, loss = 1.23805188\n",
      "Iteration 57, loss = 1.21718571\n",
      "Iteration 58, loss = 1.19623915\n",
      "Iteration 59, loss = 1.17459100\n",
      "Iteration 60, loss = 1.15269296\n",
      "Iteration 61, loss = 1.13045560\n",
      "Iteration 62, loss = 1.10837526\n",
      "Iteration 63, loss = 1.08534780\n",
      "Iteration 64, loss = 1.05949505\n",
      "Iteration 65, loss = 1.03661496\n",
      "Iteration 66, loss = 1.01356158\n",
      "Iteration 67, loss = 0.99111037\n",
      "Iteration 68, loss = 0.96773710\n",
      "Iteration 69, loss = 0.94486869\n",
      "Iteration 70, loss = 0.92203935\n",
      "Iteration 71, loss = 0.89962585\n",
      "Iteration 72, loss = 0.87708254\n",
      "Iteration 73, loss = 0.85664984\n",
      "Iteration 74, loss = 0.83718504\n",
      "Iteration 75, loss = 0.81807595\n",
      "Iteration 76, loss = 0.79999211\n",
      "Iteration 77, loss = 0.78208669\n",
      "Iteration 78, loss = 0.76594153\n",
      "Iteration 79, loss = 0.75070543\n",
      "Iteration 80, loss = 0.73523777\n",
      "Iteration 81, loss = 0.72197013\n",
      "Iteration 82, loss = 0.70824731\n",
      "Iteration 83, loss = 0.69580103\n",
      "Iteration 84, loss = 0.68506952\n",
      "Iteration 85, loss = 0.67497931\n",
      "Iteration 86, loss = 0.66314821\n",
      "Iteration 87, loss = 0.65289400\n",
      "Iteration 88, loss = 0.64389052\n",
      "Iteration 89, loss = 0.63567954\n",
      "Iteration 90, loss = 0.62696830\n",
      "Iteration 91, loss = 0.62051540\n",
      "Iteration 92, loss = 0.61385178\n",
      "Iteration 93, loss = 0.60592006\n",
      "Iteration 94, loss = 0.59947012\n",
      "Iteration 95, loss = 0.59380717\n",
      "Iteration 96, loss = 0.58778581\n",
      "Iteration 97, loss = 0.58341537\n",
      "Iteration 98, loss = 0.57723412\n",
      "Iteration 99, loss = 0.57304512\n",
      "Iteration 100, loss = 0.56837810\n",
      "Iteration 101, loss = 0.56374385\n",
      "Iteration 102, loss = 0.55951778\n",
      "Iteration 103, loss = 0.55536415\n",
      "Iteration 104, loss = 0.55137192\n",
      "Iteration 105, loss = 0.54776933\n",
      "Iteration 106, loss = 0.54479794\n",
      "Iteration 107, loss = 0.54085315\n",
      "Iteration 108, loss = 0.53763404\n",
      "Iteration 109, loss = 0.53411812\n",
      "Iteration 110, loss = 0.53179149\n",
      "Iteration 111, loss = 0.52933284\n",
      "Iteration 112, loss = 0.52557859\n",
      "Iteration 113, loss = 0.52275715\n",
      "Iteration 114, loss = 0.52024779\n",
      "Iteration 115, loss = 0.51768102\n",
      "Iteration 116, loss = 0.51460267\n",
      "Iteration 117, loss = 0.51227747\n",
      "Iteration 118, loss = 0.50981919\n",
      "Iteration 119, loss = 0.50835734\n",
      "Iteration 120, loss = 0.50638030\n",
      "Iteration 121, loss = 0.50316728\n",
      "Iteration 122, loss = 0.50090875\n",
      "Iteration 123, loss = 0.49850848\n",
      "Iteration 124, loss = 0.49675075\n",
      "Iteration 125, loss = 0.49411614\n",
      "Iteration 126, loss = 0.49312628\n",
      "Iteration 127, loss = 0.49063742\n",
      "Iteration 128, loss = 0.48801150\n",
      "Iteration 129, loss = 0.48726254\n",
      "Iteration 130, loss = 0.48523295\n",
      "Iteration 131, loss = 0.48229076\n",
      "Iteration 132, loss = 0.48143148\n",
      "Iteration 133, loss = 0.47854690\n",
      "Iteration 134, loss = 0.47788644\n",
      "Iteration 135, loss = 0.47612070\n",
      "Iteration 136, loss = 0.47283449\n",
      "Iteration 137, loss = 0.47575257\n",
      "Iteration 138, loss = 0.46970888\n",
      "Iteration 139, loss = 0.47118888\n",
      "Iteration 140, loss = 0.46678429\n",
      "Iteration 141, loss = 0.46746382\n",
      "Iteration 142, loss = 0.46489286\n",
      "Iteration 143, loss = 0.46644579\n",
      "Iteration 144, loss = 0.45944544\n",
      "Iteration 145, loss = 0.45944234\n",
      "Iteration 146, loss = 0.45919018\n",
      "Iteration 147, loss = 0.45598816\n",
      "Iteration 148, loss = 0.45447676\n",
      "Iteration 149, loss = 0.45236090\n",
      "Iteration 150, loss = 0.45136164\n",
      "Iteration 151, loss = 0.44952843\n",
      "Iteration 152, loss = 0.44828762\n",
      "Iteration 153, loss = 0.44836431\n",
      "Iteration 154, loss = 0.44530699\n",
      "Iteration 155, loss = 0.44518235\n",
      "Iteration 156, loss = 0.44362491\n",
      "Iteration 157, loss = 0.44297227\n",
      "Iteration 158, loss = 0.44124835\n",
      "Iteration 159, loss = 0.44123850\n",
      "Iteration 160, loss = 0.43863185\n",
      "Iteration 161, loss = 0.43720241\n",
      "Iteration 162, loss = 0.43654624\n",
      "Iteration 163, loss = 0.43535194\n",
      "Iteration 164, loss = 0.43441008\n",
      "Iteration 165, loss = 0.43268075\n",
      "Iteration 166, loss = 0.43256704\n",
      "Iteration 167, loss = 0.43070212\n",
      "Iteration 168, loss = 0.43010696\n",
      "Iteration 169, loss = 0.42967176\n",
      "Iteration 170, loss = 0.42850554\n",
      "Iteration 171, loss = 0.42699160\n",
      "Iteration 172, loss = 0.42607084\n",
      "Iteration 173, loss = 0.42659282\n",
      "Iteration 174, loss = 0.42482319\n",
      "Iteration 175, loss = 0.42386720\n",
      "Iteration 176, loss = 0.42316780\n",
      "Iteration 177, loss = 0.42152259\n",
      "Iteration 178, loss = 0.42125015\n",
      "Iteration 179, loss = 0.41987811\n",
      "Iteration 180, loss = 0.42014641\n",
      "Iteration 181, loss = 0.41750501\n",
      "Iteration 182, loss = 0.41879834\n",
      "Iteration 183, loss = 0.41833441\n",
      "Iteration 184, loss = 0.41624712\n",
      "Iteration 185, loss = 0.41531880\n",
      "Iteration 186, loss = 0.41519565\n",
      "Iteration 187, loss = 0.41348807\n",
      "Iteration 188, loss = 0.41309514\n",
      "Iteration 189, loss = 0.41268909\n",
      "Iteration 190, loss = 0.41115211\n",
      "Iteration 191, loss = 0.40986618\n",
      "Iteration 192, loss = 0.40938259\n",
      "Iteration 193, loss = 0.40902120\n",
      "Iteration 194, loss = 0.40780149\n",
      "Iteration 195, loss = 0.40767919\n",
      "Iteration 196, loss = 0.40674451\n",
      "Iteration 197, loss = 0.40511806\n",
      "Iteration 198, loss = 0.40481731\n",
      "Iteration 199, loss = 0.40428108\n",
      "Iteration 200, loss = 0.40297863\n",
      "Iteration 201, loss = 0.40214280\n",
      "Iteration 202, loss = 0.40190570\n",
      "Iteration 203, loss = 0.40056260\n",
      "Iteration 204, loss = 0.40015414\n",
      "Iteration 205, loss = 0.40066880\n",
      "Iteration 206, loss = 0.39879656\n",
      "Iteration 207, loss = 0.40095792\n",
      "Iteration 208, loss = 0.39783182\n",
      "Iteration 209, loss = 0.39818333\n",
      "Iteration 210, loss = 0.39875432\n",
      "Iteration 211, loss = 0.39723781\n",
      "Iteration 212, loss = 0.39589435\n",
      "Iteration 213, loss = 0.39486513\n",
      "Iteration 214, loss = 0.39340390\n",
      "Iteration 215, loss = 0.39272774\n",
      "Iteration 216, loss = 0.39228240\n",
      "Iteration 217, loss = 0.39188938\n",
      "Iteration 218, loss = 0.39115641\n",
      "Iteration 219, loss = 0.39137507\n",
      "Iteration 220, loss = 0.38952673\n",
      "Iteration 221, loss = 0.38906578\n",
      "Iteration 222, loss = 0.38781983\n",
      "Iteration 223, loss = 0.38918425\n",
      "Iteration 224, loss = 0.39006756\n",
      "Iteration 225, loss = 0.38823356\n",
      "Iteration 226, loss = 0.38703873\n",
      "Iteration 227, loss = 0.38529414\n",
      "Iteration 228, loss = 0.38464613\n",
      "Iteration 229, loss = 0.38409121\n",
      "Iteration 230, loss = 0.38340633\n",
      "Iteration 231, loss = 0.38264295\n",
      "Iteration 232, loss = 0.38271896\n",
      "Iteration 233, loss = 0.38619853\n",
      "Iteration 234, loss = 0.38293738\n",
      "Iteration 235, loss = 0.38386713\n",
      "Iteration 236, loss = 0.37984078\n",
      "Iteration 237, loss = 0.37994562\n",
      "Iteration 238, loss = 0.37861886\n",
      "Iteration 239, loss = 0.37929737\n",
      "Iteration 240, loss = 0.37866640\n",
      "Iteration 241, loss = 0.37854525\n",
      "Iteration 242, loss = 0.37771307\n",
      "Iteration 243, loss = 0.37909722\n",
      "Iteration 244, loss = 0.37765638\n",
      "Iteration 245, loss = 0.38088477\n",
      "Iteration 246, loss = 0.37978916\n",
      "Iteration 247, loss = 0.37602571\n",
      "Iteration 248, loss = 0.37469692\n",
      "Iteration 249, loss = 0.37340130\n",
      "Iteration 250, loss = 0.37508534\n",
      "Iteration 251, loss = 0.37117519\n",
      "Iteration 252, loss = 0.37703915\n",
      "Iteration 253, loss = 0.37551273\n",
      "Iteration 254, loss = 0.37378768\n",
      "Iteration 255, loss = 0.37086417\n",
      "Iteration 256, loss = 0.37089489\n",
      "Iteration 257, loss = 0.36890134\n",
      "Iteration 258, loss = 0.36937403\n",
      "Iteration 259, loss = 0.36763318\n",
      "Iteration 260, loss = 0.36859030\n",
      "Iteration 261, loss = 0.36782940\n",
      "Iteration 262, loss = 0.36646837\n",
      "Iteration 263, loss = 0.36649619\n",
      "Iteration 264, loss = 0.36543698\n",
      "Iteration 265, loss = 0.36542647\n",
      "Iteration 266, loss = 0.36514694\n",
      "Iteration 267, loss = 0.36508209\n",
      "Iteration 268, loss = 0.36398282\n",
      "Iteration 269, loss = 0.36352603\n",
      "Iteration 270, loss = 0.36373809\n",
      "Iteration 271, loss = 0.36249392\n",
      "Iteration 272, loss = 0.36286378\n",
      "Iteration 273, loss = 0.36259229\n",
      "Iteration 274, loss = 0.36621375\n",
      "Iteration 275, loss = 0.36296016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 276, loss = 0.36397396\n",
      "Iteration 277, loss = 0.36015721\n",
      "Iteration 278, loss = 0.36156517\n",
      "Iteration 279, loss = 0.35984708\n",
      "Iteration 280, loss = 0.36060824\n",
      "Iteration 281, loss = 0.36029713\n",
      "Iteration 282, loss = 0.35896598\n",
      "Iteration 283, loss = 0.35737305\n",
      "Iteration 284, loss = 0.35951839\n",
      "Iteration 285, loss = 0.35913867\n",
      "Iteration 286, loss = 0.35607695\n",
      "Iteration 287, loss = 0.35844780\n",
      "Iteration 288, loss = 0.35616384\n",
      "Iteration 289, loss = 0.35791764\n",
      "Iteration 290, loss = 0.35508107\n",
      "Iteration 291, loss = 0.35624476\n",
      "Iteration 292, loss = 0.35468881\n",
      "Iteration 293, loss = 0.35610367\n",
      "Iteration 294, loss = 0.35286563\n",
      "Iteration 295, loss = 0.35306794\n",
      "Iteration 296, loss = 0.35311371\n",
      "Iteration 297, loss = 0.35214569\n",
      "Iteration 298, loss = 0.35189201\n",
      "Iteration 299, loss = 0.35373345\n",
      "Iteration 300, loss = 0.35105554\n",
      "Iteration 301, loss = 0.35085733\n",
      "Iteration 302, loss = 0.35224599\n",
      "Iteration 303, loss = 0.35051356\n",
      "Iteration 304, loss = 0.34961429\n",
      "Iteration 305, loss = 0.34948456\n",
      "Iteration 306, loss = 0.34875897\n",
      "Iteration 307, loss = 0.34991036\n",
      "Iteration 308, loss = 0.34928546\n",
      "Iteration 309, loss = 0.34818700\n",
      "Iteration 310, loss = 0.34765422\n",
      "Iteration 311, loss = 0.34736078\n",
      "Iteration 312, loss = 0.34648205\n",
      "Iteration 313, loss = 0.34621480\n",
      "Iteration 314, loss = 0.34641013\n",
      "Iteration 315, loss = 0.34576000\n",
      "Iteration 316, loss = 0.34530207\n",
      "Iteration 317, loss = 0.34543678\n",
      "Iteration 318, loss = 0.34434039\n",
      "Iteration 319, loss = 0.34487992\n",
      "Iteration 320, loss = 0.34420741\n",
      "Iteration 321, loss = 0.34385639\n",
      "Iteration 322, loss = 0.34402468\n",
      "Iteration 323, loss = 0.34352907\n",
      "Iteration 324, loss = 0.34361093\n",
      "Iteration 325, loss = 0.34199005\n",
      "Iteration 326, loss = 0.34205699\n",
      "Iteration 327, loss = 0.34185012\n",
      "Iteration 328, loss = 0.34411314\n",
      "Iteration 329, loss = 0.34325793\n",
      "Iteration 330, loss = 0.34175596\n",
      "Iteration 331, loss = 0.33978110\n",
      "Iteration 332, loss = 0.34297252\n",
      "Iteration 333, loss = 0.34211998\n",
      "Iteration 334, loss = 0.34091433\n",
      "Iteration 335, loss = 0.33905782\n",
      "Iteration 336, loss = 0.34080966\n",
      "Iteration 337, loss = 0.34026439\n",
      "Iteration 338, loss = 0.34056744\n",
      "Iteration 339, loss = 0.34170415\n",
      "Iteration 340, loss = 0.33762490\n",
      "Iteration 341, loss = 0.33773943\n",
      "Iteration 342, loss = 0.33730055\n",
      "Iteration 343, loss = 0.34003386\n",
      "Iteration 344, loss = 0.33730968\n",
      "Iteration 345, loss = 0.34162499\n",
      "Iteration 346, loss = 0.33458906\n",
      "Iteration 347, loss = 0.33882255\n",
      "Iteration 348, loss = 0.33895972\n",
      "Iteration 349, loss = 0.33522806\n",
      "Iteration 350, loss = 0.33516896\n",
      "Iteration 351, loss = 0.33441867\n",
      "Iteration 352, loss = 0.33544929\n",
      "Iteration 353, loss = 0.33412847\n",
      "Iteration 354, loss = 0.33464153\n",
      "Iteration 355, loss = 0.33337383\n",
      "Iteration 356, loss = 0.33332302\n",
      "Iteration 357, loss = 0.33268019\n",
      "Iteration 358, loss = 0.33318509\n",
      "Iteration 359, loss = 0.33277673\n",
      "Iteration 360, loss = 0.33243842\n",
      "Iteration 361, loss = 0.33204355\n",
      "Iteration 362, loss = 0.33121567\n",
      "Iteration 363, loss = 0.33072319\n",
      "Iteration 364, loss = 0.33112456\n",
      "Iteration 365, loss = 0.33187207\n",
      "Iteration 366, loss = 0.33068965\n",
      "Iteration 367, loss = 0.33083236\n",
      "Iteration 368, loss = 0.33043194\n",
      "Iteration 369, loss = 0.32971169\n",
      "Iteration 370, loss = 0.32981778\n",
      "Iteration 371, loss = 0.32908666\n",
      "Iteration 372, loss = 0.32974317\n",
      "Iteration 373, loss = 0.32893667\n",
      "Iteration 374, loss = 0.32822762\n",
      "Iteration 375, loss = 0.32792775\n",
      "Iteration 376, loss = 0.32767255\n",
      "Iteration 377, loss = 0.33145263\n",
      "Iteration 378, loss = 0.32719742\n",
      "Iteration 379, loss = 0.32886118\n",
      "Iteration 380, loss = 0.32725538\n",
      "Iteration 381, loss = 0.32618161\n",
      "Iteration 382, loss = 0.32709968\n",
      "Iteration 383, loss = 0.32585576\n",
      "Iteration 384, loss = 0.32787142\n",
      "Iteration 385, loss = 0.32770057\n",
      "Iteration 386, loss = 0.32582638\n",
      "Iteration 387, loss = 0.32521818\n",
      "Iteration 388, loss = 0.32471258\n",
      "Iteration 389, loss = 0.32487722\n",
      "Iteration 390, loss = 0.32399707\n",
      "Iteration 391, loss = 0.32398289\n",
      "Iteration 392, loss = 0.32422111\n",
      "Iteration 393, loss = 0.32476551\n",
      "Iteration 394, loss = 0.32447117\n",
      "Iteration 395, loss = 0.32445806\n",
      "Iteration 396, loss = 0.32177743\n",
      "Iteration 397, loss = 0.32456231\n",
      "Iteration 398, loss = 0.32499335\n",
      "Iteration 399, loss = 0.32303790\n",
      "Iteration 400, loss = 0.32151086\n",
      "Iteration 401, loss = 0.32246777\n",
      "Iteration 402, loss = 0.32190802\n",
      "Iteration 403, loss = 0.32100748\n",
      "Iteration 404, loss = 0.32316905\n",
      "Iteration 405, loss = 0.32121305\n",
      "Iteration 406, loss = 0.32128406\n",
      "Iteration 407, loss = 0.32060363\n",
      "Iteration 408, loss = 0.31943941\n",
      "Iteration 409, loss = 0.32062689\n",
      "Iteration 410, loss = 0.31932168\n",
      "Iteration 411, loss = 0.32003491\n",
      "Iteration 412, loss = 0.31929872\n",
      "Iteration 413, loss = 0.31929207\n",
      "Iteration 414, loss = 0.32048494\n",
      "Iteration 415, loss = 0.32030035\n",
      "Iteration 416, loss = 0.31920620\n",
      "Iteration 417, loss = 0.31941015\n",
      "Iteration 418, loss = 0.31692546\n",
      "Iteration 419, loss = 0.32032327\n",
      "Iteration 420, loss = 0.31787754\n",
      "Iteration 421, loss = 0.32042526\n",
      "Iteration 422, loss = 0.31616312\n",
      "Iteration 423, loss = 0.32072929\n",
      "Iteration 424, loss = 0.32036817\n",
      "Iteration 425, loss = 0.31598356\n",
      "Iteration 426, loss = 0.31861244\n",
      "Iteration 427, loss = 0.31601407\n",
      "Iteration 428, loss = 0.31914710\n",
      "Iteration 429, loss = 0.31427409\n",
      "Iteration 430, loss = 0.31931526\n",
      "Iteration 431, loss = 0.31704484\n",
      "Iteration 432, loss = 0.31424776\n",
      "Iteration 433, loss = 0.31692381\n",
      "Iteration 434, loss = 0.31542848\n",
      "Iteration 435, loss = 0.31468051\n",
      "Iteration 436, loss = 0.31349327\n",
      "Iteration 437, loss = 0.31457329\n",
      "Iteration 438, loss = 0.31324342\n",
      "Iteration 439, loss = 0.31349756\n",
      "Iteration 440, loss = 0.31572697\n",
      "Iteration 441, loss = 0.31407356\n",
      "Iteration 442, loss = 0.31387353\n",
      "Iteration 443, loss = 0.31277077\n",
      "Iteration 444, loss = 0.31368554\n",
      "Iteration 445, loss = 0.31243613\n",
      "Iteration 446, loss = 0.31168036\n",
      "Iteration 447, loss = 0.31121319\n",
      "Iteration 448, loss = 0.31155822\n",
      "Iteration 449, loss = 0.31078380\n",
      "Iteration 450, loss = 0.31260243\n",
      "Iteration 451, loss = 0.31203360\n",
      "Iteration 452, loss = 0.31176085\n",
      "Iteration 453, loss = 0.31205162\n",
      "Iteration 454, loss = 0.30992906\n",
      "Iteration 455, loss = 0.31193808\n",
      "Iteration 456, loss = 0.31252483\n",
      "Iteration 457, loss = 0.31316171\n",
      "Iteration 458, loss = 0.31038663\n",
      "Iteration 459, loss = 0.30843772\n",
      "Iteration 460, loss = 0.30980887\n",
      "Iteration 461, loss = 0.30898195\n",
      "Iteration 462, loss = 0.30930689\n",
      "Iteration 463, loss = 0.30856051\n",
      "Iteration 464, loss = 0.30871123\n",
      "Iteration 465, loss = 0.31031500\n",
      "Iteration 466, loss = 0.30761744\n",
      "Iteration 467, loss = 0.30903231\n",
      "Iteration 468, loss = 0.30945763\n",
      "Iteration 469, loss = 0.30876068\n",
      "Iteration 470, loss = 0.30725987\n",
      "Iteration 471, loss = 0.30825253\n",
      "Iteration 472, loss = 0.30536964\n",
      "Iteration 473, loss = 0.30687337\n",
      "Iteration 474, loss = 0.30915068\n",
      "Iteration 475, loss = 0.30562084\n",
      "Iteration 476, loss = 0.30876641\n",
      "Iteration 477, loss = 0.30756860\n",
      "Iteration 478, loss = 0.30584612\n",
      "Iteration 479, loss = 0.30669230\n",
      "Iteration 480, loss = 0.30525012\n",
      "Iteration 481, loss = 0.30456177\n",
      "Iteration 482, loss = 0.30455108\n",
      "Iteration 483, loss = 0.30476449\n",
      "Iteration 484, loss = 0.30430343\n",
      "Iteration 485, loss = 0.30446606\n",
      "Iteration 486, loss = 0.30389203\n",
      "Iteration 487, loss = 0.30322527\n",
      "Iteration 488, loss = 0.30604904\n",
      "Iteration 489, loss = 0.30217175\n",
      "Iteration 490, loss = 0.30450021\n",
      "Iteration 491, loss = 0.30576812\n",
      "Iteration 492, loss = 0.30158963\n",
      "Iteration 493, loss = 0.30728604\n",
      "Iteration 494, loss = 0.30494556\n",
      "Iteration 495, loss = 0.30198427\n",
      "Iteration 496, loss = 0.30336804\n",
      "Iteration 497, loss = 0.30284223\n",
      "Iteration 498, loss = 0.30238989\n",
      "Iteration 499, loss = 0.30219584\n",
      "Iteration 500, loss = 0.30147463\n",
      "Accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mariobs/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(48,26,12),max_iter=500,solver='adam',verbose=True,\n",
    "                    n_iter_no_change=35,activation=\"relu\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xtrain,Ytrain,random_state=1, test_size=0.2)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "Y_pred=clf.predict(X_test)\n",
    "#print(clf.score(X_testscaled, Y_test))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
